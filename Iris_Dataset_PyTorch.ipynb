{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhQhyOtGEtm08sEFD+PZgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PJunior17/-Friendly-Comprehensive-Guide-to-Computer-Vision-with-PyTorch/blob/main/Iris_Dataset_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This is the first project based on the book: Computer Vision with Neural Networks from Zero to Hero by Aman Urumbekov**\n",
        "\n",
        "I had found this book via a reddit post on r/computervison\n",
        "https://www.reddit.com/r/computervision/comments/1853skm/writing_a_book_on_computer_vision_with_pytorch/"
      ],
      "metadata": {
        "id": "M8vV84XsfCwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "c8sFpwkvik47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #main pytorch library\n",
        "import torch.nn as nn #pytorch module for building neural network layers\n",
        "import torch.optim as optim #module to provide optimization algorithms like SGD and Adam\n",
        "\n",
        "from sklearn.datasets import load_iris #function to load the iris dataset\n",
        "from sklearn.model_selection import train_test_split #utility to split the dataset into training and testing\n",
        "from sklearn.preprocessing import StandardScaler #preprocessing tool to standardize our features (scaling them to have zero mean and unit variance to perform better)"
      ],
      "metadata": {
        "id": "4zcVg4DZIEAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Dataset"
      ],
      "metadata": {
        "id": "yYZo18V0mnd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n"
      ],
      "metadata": {
        "id": "HeACWIPBm0o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Train Split"
      ],
      "metadata": {
        "id": "DsoEyzLlHfc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=17)"
      ],
      "metadata": {
        "id": "Q3wRYQXwISUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing the Data"
      ],
      "metadata": {
        "id": "GI43aB0OIrVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler() #create an instance of StandardScal\n",
        "Xtrain = scaler.fit_transform(Xtrain) #computes the mean and standard deviation of each feature in the training set and then scales the training data accordingly\n",
        "Xtest = scaler.transform(Xtest) #applies the same transformation to the testing data\n",
        "\n",
        "#more to read abbout fit_transform() and transform() https://chat.openai.com/c/a23c2fe1-6b6f-4168-898e-866fcafe3b81"
      ],
      "metadata": {
        "id": "GCekO6uXIutM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Data to PyTorch Tensors"
      ],
      "metadata": {
        "id": "ESP2L02fKEoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the features are all floats in the dataset so we need to convert to float tensors\n",
        "Xtrain = torch.FloatTensor(Xtrain)\n",
        "Xtest = torch.FloatTensor(Xtest)\n",
        "\n",
        "#the labels are all integers in the dataset so we need to convert to long tensors\n",
        "ytrain = torch.LongTensor(ytrain)\n",
        "ytest = torch.LongTensor(ytest)\n"
      ],
      "metadata": {
        "id": "d0cmBmmvKQPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Perceptron"
      ],
      "metadata": {
        "id": "lTacKTysKoSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A perceptron is a single-layer neural network, the most basic form of a classifier\n",
        "\n",
        "class Perceptron(nn.Module): #define a class named Perceptron that inherits from nn.Module, the base class for all neural networks in PyTorch\n",
        "  def __init__(self, input_dim): #this method initializes the superclass and defines the layers of the network\n",
        "    super(Perceptron, self).__init__()\n",
        "    self.fc = nn.Linear(in_features=input_dim,\n",
        "                out_features=3) #a fully connected linear layer which takes the the size at each input sample and the amount of outputs we expect\n",
        "\n",
        "  def forward(self, x): #this methoud defines how the data flows through the network\n",
        "    return self.fc(x)"
      ],
      "metadata": {
        "id": "tUwSS4bLLEvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Perceptron Model and Setup Training"
      ],
      "metadata": {
        "id": "YNj429Q6L4NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Perceptron(input_dim=4)"
      ],
      "metadata": {
        "id": "Y0ZBXDjlNEhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss() #this loss function is for multi-class problems.\n",
        "                                  #CrossEntropyLoss() measures the performance of a classification model whose output is a probablity value between 0 and 1"
      ],
      "metadata": {
        "id": "xVPygwYkNHcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-2) #Adam is a popular choice for many deep learning applications\n",
        "                                                    #It is known for its efficiency in handling sparse gradients and its adaptiveness"
      ],
      "metadata": {
        "id": "LWuUuiWCNw9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "xXp6E8wnOFbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "  #forward propagation\n",
        "  #computes the predicted outputs by passing the training data through the model\n",
        "  outputs = model(Xtrain)\n",
        "  loss = criterion(outputs, ytrain) #calculates the loss\n",
        "\n",
        "  #backward propagation\n",
        "  #Computes the gradient of the loss with respect to the model parameters\n",
        "  optimizer.zero_grad() #clear the gradient of all optimized values\n",
        "  loss.backward() #performs the back propagation\n",
        "\n",
        "  #performs a single optimization step\n",
        "  optimizer.step()\n",
        "\n",
        "  #print loss every 10 epochs\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print('Epoch: %s | Loss: %s' % (epoch, loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scbaThfdOVE0",
        "outputId": "ce1f6976-201b-4e52-c484-1587400519e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9 | Loss: tensor(1.3364, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 19 | Loss: tensor(1.0141, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 29 | Loss: tensor(0.7990, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 39 | Loss: tensor(0.6678, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 49 | Loss: tensor(0.5819, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 59 | Loss: tensor(0.5211, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 69 | Loss: tensor(0.4766, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 79 | Loss: tensor(0.4429, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 89 | Loss: tensor(0.4165, grad_fn=<NllLossBackward0>)\n",
            "Epoch: 99 | Loss: tensor(0.3951, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model"
      ],
      "metadata": {
        "id": "pTHuhzedPciR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #setting the mode to evaluate\n",
        "\n",
        "correct = 0 #counter for correct prediction\n",
        "total = 0 #counter for total predictions\n",
        "outputs = model(Xtest) #computes the prediction on the test set\n",
        "\n",
        "_, predicted = torch.max(outputs, 1) #selects the class with the highest score as our prediction\n",
        "\n",
        "#update the total and correct counts\n",
        "total += ytest.shape[0]\n",
        "correct += (predicted == ytest).sum().item()\n",
        "\n",
        "print('Accuracy: %s' % (100*correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeK5eL6BPtIs",
        "outputId": "87c5d2a9-d514-44c8-e4b7-0b8e321d4dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 93.33333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ehd91bizQlZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}